{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "764c3451-86b1-4052-a722-304896678bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5402e4bd-c865-47da-89c5-81f457b7eba1",
   "metadata": {},
   "source": [
    "# Section 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc733ed1-bc41-4a27-a0aa-e0f3eadf06b5",
   "metadata": {},
   "source": [
    "### a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35d264a-b2a4-46b9-a2d1-94e59d77c696",
   "metadata": {},
   "source": [
    "If $c \\approx v_j$, then $\\alpha_j \\approx 1$ and $\\alpha_i \\approx 0$, for $i \\neq j$. That is, the dot product between $k_j$ and $q$ must be much larger than the dot product between $k_i$ and $q$ for $i \\neq j$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd402588-00a1-4949-97ce-306ed0fbe398",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de66a3f-a510-470d-836f-2ec911586d80",
   "metadata": {},
   "source": [
    "For $c \\approx \\frac{1}{2}(v_a+v_b)$, it is enough that $\\alpha_i \\approx \\frac{1}{2}$, for $i \\in \\{a, b\\}$, and $\\alpha_j \\approx 0$, for $j \\not\\in \\{a,b\\}$.<br>\n",
    "This can be achieved when $q = (10^{10}+n)\\cdot(k_a + k_b)$.<br>\n",
    "Then\n",
    "$$k_i^T\\cdot q = k_i^T\\cdot (10^{10}+n)\\cdot(k_a + k_b) = 10^{10}+n,$$ \n",
    "and \n",
    "$$\\alpha_i = \\frac{\\exp(10^{10}+n)}{\\exp(10^{10}+n)+\\exp(10^{10}+n)+n-2}\\approx \\frac{1}{2},$$\n",
    "for $i \\in \\{a, b\\}$,<br>\n",
    "as well as  \n",
    "$$k_j^T\\cdot q = k_j^T\\cdot (10^{10}+n)\\cdot(k_a, k_b) = 0,$$\n",
    "and \n",
    "$$\\alpha_j = \\frac{\\exp(0)}{\\exp(10^{10}+n)+\\exp(10^{10}+n)+n-2} = \\frac{1}{\\exp(10^{10}+n)+\\exp(10^{10}+n)+n-2} \\approx 0,$$\n",
    "for $i \\not\\in \\{a, b\\}$.<br>\n",
    "Therefore  $$c = \\sum_{i=1}^n\\alpha_i\\cdot v_i \\approx \\frac{1}{2}(v_a+v_b).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd446c6f-8e67-47b2-b43a-057b2adc7e27",
   "metadata": {},
   "source": [
    "### c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e5fec2-6413-4df1-bad3-cf03e526375b",
   "metadata": {},
   "source": [
    "##### I)\n",
    "For $c \\approx \\frac{1}{2}(v_a+v_b)$\n",
    "$$\\alpha_i = \\frac{\\exp((10^{10}+n)(k_i^T\\cdot k_a + k_i^T\\cdot k_b))}{S},$$\n",
    "where $S = \\sum_{j=1}^n \\exp((10^{10}+n)(k_j^T\\cdot k_a + k_j^T\\cdot k_b))$.\n",
    "\n",
    "We have \n",
    "$$\\large P(||k_i-\\mu_i||^2>\\epsilon) < \\frac{Var(||k_i-\\mu_i||^2)}{\\epsilon^2} = \\frac{d\\cdot \\alpha}{\\epsilon^2} \\xrightarrow[\\alpha \\to 0]{} 0,$$\n",
    "where $d$ is a dimensionality of $k_i$.<br>\n",
    "Therefore, as before we get that\n",
    "$$c = \\sum_{i=1}^n\\alpha_i\\cdot v_i \\approx \\frac{1}{2}(v_a+v_b).$$\n",
    "\n",
    "##### II)\n",
    "An additional component that is added to the covariation matrix makes it impossible to say that $k_i$ is very similar to $u_i$, as in point $I)$. Using $q$ from point $I)$, I expect the attention weights to be approximately uniformly distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b5986b-8e36-4093-a657-39da49a16328",
   "metadata": {},
   "source": [
    "### e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174bf5c2-c781-499d-a531-d1392d5a62c7",
   "metadata": {},
   "source": [
    "##### I)\n",
    "Part 1<br>\n",
    "$c_2 = \\sum_{j=1}^n\\alpha_{2j}\\cdot v_j$\n",
    "We have<br>\n",
    "$k_1^T\\cdot q_2 = x_1^T\\cdot x_2 = (u_d + u_b)^T\\cdot u_a = 0$,<br>\n",
    "$k_2^T\\cdot q_2 = x_2^T\\cdot x_2 = u_a^T\\cdot u_a = \\beta$,<br>\n",
    "$k_3^T\\cdot q_2 = x_3^T\\cdot x_2 = (u_c + u_b)^T\\cdot u_a = 0$,<br>\n",
    "therefore<br>\n",
    "$\\large\\alpha_{21} = \\frac{1}{2+\\exp(\\beta)}$,<br>\n",
    "$\\large\\alpha_{22} = \\frac{\\beta}{2+\\exp(\\beta)}$,<br>\n",
    "$\\large\\alpha_{23} = \\frac{1}{2+\\exp(\\beta)}$,<br>\n",
    "and\n",
    "$$\\large c_2 = \\frac{v_1 + v_3 + \\exp(\\beta)\\cdot v_2}{2+\\exp(\\beta)}\\approx v_2 = x_2$$\n",
    "\n",
    "Part 2<br>\n",
    "No, because adding either $u_d$ or $u_c$ to $x_2$ automatically creates at least two big attention weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e259481-1318-437a-ba52-671a2383cd0a",
   "metadata": {},
   "source": [
    "# Section 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194d1225-6b03-4091-81b5-ebdaab7d3faf",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e49c577-3c3a-471c-b9d8-989aa9cf88bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 418352 characters, 256 unique.\n",
      "x: Where was Khatchig Mouradian born?⁇Lebanon⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y: □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□⁇Lebanon⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "x: Where was Jacob Henry Studer born?⁇Columbus⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y: □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□⁇Columbus⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "x: Where was John Stephen born?⁇Glasgow⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y: □□□□□□□□□□□□□□□□□□□□□□□□□□□⁇Glasgow⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "x: Where was Georgina Willis born?⁇Australia⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y: □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□⁇Australia⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n"
     ]
    }
   ],
   "source": [
    "! python src/dataset.py namedata "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd1871e-41c9-44cf-8377-470beba033db",
   "metadata": {},
   "source": [
    "### d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f202f9a-3d74-46c4-a16b-605a3177e952",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 418352 characters, 256 unique.\n",
      "epoch 1 iter 7: train loss 0.88346. lr 5.999844e-04: 100%|█| 8/8 [00:04<00:00,  \n",
      "epoch 2 iter 7: train loss 0.53916. lr 5.999351e-04: 100%|█| 8/8 [00:04<00:00,  \n",
      "epoch 3 iter 7: train loss 0.40441. lr 5.998521e-04: 100%|█| 8/8 [00:04<00:00,  \n",
      "epoch 4 iter 7: train loss 0.30100. lr 5.997352e-04: 100%|█| 8/8 [00:04<00:00,  \n",
      "epoch 5 iter 7: train loss 0.26342. lr 5.995847e-04: 100%|█| 8/8 [00:04<00:00,  \n",
      "epoch 6 iter 7: train loss 0.24267. lr 5.994004e-04: 100%|█| 8/8 [00:04<00:00,  \n",
      "epoch 7 iter 7: train loss 0.23243. lr 5.991823e-04: 100%|█| 8/8 [00:04<00:00,  \n",
      "epoch 8 iter 7: train loss 0.22493. lr 5.989306e-04: 100%|█| 8/8 [00:04<00:00,  \n",
      "epoch 9 iter 7: train loss 0.21411. lr 5.986453e-04: 100%|█| 8/8 [00:04<00:00,  \n",
      "epoch 10 iter 7: train loss 0.19609. lr 5.983263e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 11 iter 7: train loss 0.18515. lr 5.979737e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 12 iter 7: train loss 0.17742. lr 5.975876e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 13 iter 7: train loss 0.17215. lr 5.971680e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 14 iter 7: train loss 0.16838. lr 5.967149e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 15 iter 7: train loss 0.16339. lr 5.962284e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 16 iter 7: train loss 0.16111. lr 5.957086e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 17 iter 7: train loss 0.15830. lr 5.951554e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 18 iter 7: train loss 0.15763. lr 5.945690e-04: 100%|█| 8/8 [00:05<00:00, \n",
      "epoch 19 iter 7: train loss 0.15466. lr 5.939495e-04: 100%|█| 8/8 [00:05<00:00, \n",
      "epoch 20 iter 7: train loss 0.15225. lr 5.932969e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 21 iter 7: train loss 0.15027. lr 5.926112e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 22 iter 7: train loss 0.14784. lr 5.918926e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 23 iter 7: train loss 0.14582. lr 5.911412e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 24 iter 7: train loss 0.14273. lr 5.903569e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 25 iter 7: train loss 0.14005. lr 5.895400e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 26 iter 7: train loss 0.13791. lr 5.886905e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 27 iter 7: train loss 0.13609. lr 5.878084e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 28 iter 7: train loss 0.13301. lr 5.868940e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 29 iter 7: train loss 0.12942. lr 5.859473e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 30 iter 7: train loss 0.12609. lr 5.849683e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 31 iter 7: train loss 0.12433. lr 5.839573e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 32 iter 7: train loss 0.11925. lr 5.829143e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 33 iter 7: train loss 0.11714. lr 5.818395e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 34 iter 7: train loss 0.11338. lr 5.807329e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 35 iter 7: train loss 0.11033. lr 5.795947e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 36 iter 7: train loss 0.10845. lr 5.784251e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 37 iter 7: train loss 0.10646. lr 5.772241e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 38 iter 7: train loss 0.10338. lr 5.759918e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 39 iter 7: train loss 0.10086. lr 5.747285e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 40 iter 7: train loss 0.09743. lr 5.734343e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 41 iter 7: train loss 0.09398. lr 5.721093e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 42 iter 7: train loss 0.09099. lr 5.707537e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 43 iter 7: train loss 0.08884. lr 5.693675e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 44 iter 7: train loss 0.08454. lr 5.679511e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 45 iter 7: train loss 0.08269. lr 5.665044e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 46 iter 7: train loss 0.08129. lr 5.650278e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 47 iter 7: train loss 0.07784. lr 5.635213e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 48 iter 7: train loss 0.07742. lr 5.619852e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 49 iter 7: train loss 0.07596. lr 5.604195e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 50 iter 7: train loss 0.07355. lr 5.588246e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 51 iter 7: train loss 0.07265. lr 5.572005e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 52 iter 7: train loss 0.07095. lr 5.555474e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 53 iter 7: train loss 0.06838. lr 5.538656e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 54 iter 7: train loss 0.06546. lr 5.521552e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 55 iter 7: train loss 0.06674. lr 5.504164e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 56 iter 7: train loss 0.06515. lr 5.486494e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 57 iter 7: train loss 0.06139. lr 5.468544e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 58 iter 7: train loss 0.06353. lr 5.450316e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 59 iter 7: train loss 0.06142. lr 5.431812e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 60 iter 7: train loss 0.06085. lr 5.413034e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 61 iter 7: train loss 0.05922. lr 5.393985e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 62 iter 7: train loss 0.05767. lr 5.374666e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 63 iter 7: train loss 0.05835. lr 5.355080e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 64 iter 7: train loss 0.05639. lr 5.335229e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 65 iter 7: train loss 0.05434. lr 5.315115e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 66 iter 7: train loss 0.05398. lr 5.294740e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 67 iter 7: train loss 0.05409. lr 5.274107e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 68 iter 7: train loss 0.05280. lr 5.253217e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 69 iter 7: train loss 0.05174. lr 5.232074e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 70 iter 7: train loss 0.05361. lr 5.210680e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 71 iter 7: train loss 0.05193. lr 5.189037e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 72 iter 7: train loss 0.05045. lr 5.167147e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 73 iter 7: train loss 0.04853. lr 5.145014e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 74 iter 7: train loss 0.04764. lr 5.122639e-04: 100%|█| 8/8 [00:04<00:00, \n",
      "epoch 75 iter 7: train loss 0.04846. lr 5.100024e-04: 100%|█| 8/8 [00:04<00:00, \n"
     ]
    }
   ],
   "source": [
    "# Train on the names dataset\n",
    "!python src/run.py finetune vanilla wiki.txt \\\n",
    "--writing_params_path vanilla.model.params \\\n",
    "--finetune_corpus_path birth_places_train.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "943c30a8-2f21-4962-9f4e-99edbb442c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 418352 characters, 256 unique.\n",
      "500it [00:30, 16.14it/s]\n",
      "Correct: 5.0 out of 500.0: 1.0%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the dev set, writing out predictions\n",
    "!python src/run.py evaluate vanilla wiki.txt \\\n",
    "--reading_params_path vanilla.model.params \\\n",
    "--eval_corpus_path birth_dev.tsv \\\n",
    "--outputs_path vanilla.nopretrain.dev.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75511612-3c1c-4ce1-bcae-83b3ce5165ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 25.0 out of 500.0: 5.0%\n"
     ]
    }
   ],
   "source": [
    "#london baseline\n",
    "from src.london_baseline import evaluate_places_london_baseline\n",
    "\n",
    "total, correct = evaluate_places_london_baseline('birth_dev.tsv')\n",
    "if total > 0:\n",
    "    print('Correct: {} out of {}: {}%'.format(correct, total, correct/total*100))\n",
    "else:\n",
    "    print('Predictions written to {}; no targets provided'\n",
    "            .format(args.outputs_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8a97f1-9ff4-47e0-97b3-9da693efc218",
   "metadata": {},
   "source": [
    "### e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "978118c9-3507-4018-be4e-e4f537819c38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 418352 characters, 256 unique.\n",
      "x: Khatchig Mouradian. Khatchig Mouradian i⁇riter and tra⁇s a journalist, w□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y: hatchig Mouradian. Khatchig Mouradian i⁇riter and tra⁇s a journalist, w□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "x: Jacob Henry Studer. Jacob Henry Studer (26 Febr⁇gust 1904 New York City) was a pri⁇uary 1840 Columbus, Ohio - 2 Au□□□□□□□□□□□□□□\n",
      "y: acob Henry Studer. Jacob Henry Studer (26 Febr⁇gust 1904 New York City) was a pri⁇uary 1840 Columbus, Ohio - 2 Au□□□□□□□□□□□□□□□\n",
      "x: Jo⁇Glasgow, Stephen became a welder's apprentice on leaving school .⁇hn Stephen. Born in □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y: o⁇Glasgow, Stephen became a welder's apprentice on leaving school .⁇hn Stephen. Born in □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "x: Georgina Willis. Georgina Willis is a⁇ctor who was born in Au⁇n award winning film dire□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y: eorgina Willis. Georgina Willis is a⁇ctor who was born in Au⁇n award winning film dire□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "x: Stanley Corrsin.⁇orn on 3 April 1920 in Philadelphia, Pennsylvania .⁇ Corrsin was b□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y: tanley Corrsin.⁇orn on 3 April 1920 in Philadelphia, Pennsylvania .⁇ Corrsin was b□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "x: Eduard Ender. Eduard Ender ⁇ember 1883 London) was an Austrian pai⁇(3 March 1822 Rome -- 28 Dec□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y: duard Ender. Eduard Ender ⁇ember 1883 London) was an Austrian pai⁇(3 March 1822 Rome -- 28 Dec□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "x: John Fisher. Born in Glasgow, ⁇duate of t⁇Fisher is a gra□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y: ohn Fisher. Born in Glasgow, ⁇duate of t⁇Fisher is a gra□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "x: Iago Dekanozishvili. Iago Dekanozishvili (born in 1951 in Tbilisi)⁇ graduate⁇ is a Georgian artist, a□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y: ago Dekanozishvili. Iago Dekanozishvili (born in 1951 in Tbilisi)⁇ graduate⁇ is a Georgian artist, a□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "x: Lucy Toulmin ⁇ Smith was born at Boston, Massachuset⁇Smith. Lucy Toulmin□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y: ucy Toulmin ⁇ Smith was born at Boston, Massachuset⁇Smith. Lucy Toulmin□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "x: George Osborn. Osborn was born at Roch⁇⁇ester in 1808 .□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y: eorge Osborn. Osborn was born at Roch⁇⁇ester in 1808 .□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n"
     ]
    }
   ],
   "source": [
    "! python src/dataset.py charcorruption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286351b7-d0d9-46e1-ae76-61b459ea9523",
   "metadata": {},
   "source": [
    "### f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b5b33d3-f6aa-49de-b693-db51369a77ed",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 418352 characters, 256 unique.\n",
      "epoch 1 iter 22: train loss 1.62883. lr 5.999650e-03: 100%|█| 23/23 [00:06<00:00\n",
      "epoch 2 iter 22: train loss 1.52870. lr 5.998559e-03: 100%|█| 23/23 [00:07<00:00\n",
      "epoch 3 iter 22: train loss 1.49026. lr 5.996729e-03: 100%|█| 23/23 [00:07<00:00\n",
      "epoch 4 iter 22: train loss 1.29071. lr 5.994159e-03: 100%|█| 23/23 [00:07<00:00\n",
      "epoch 5 iter 22: train loss 1.29110. lr 5.990850e-03: 100%|█| 23/23 [00:07<00:00\n",
      "epoch 6 iter 22: train loss 1.34957. lr 5.986803e-03: 100%|█| 23/23 [00:06<00:00\n",
      "epoch 7 iter 22: train loss 1.18777. lr 5.982019e-03: 100%|█| 23/23 [00:07<00:00\n",
      "epoch 8 iter 22: train loss 1.24277. lr 5.976498e-03: 100%|█| 23/23 [00:06<00:00\n",
      "epoch 9 iter 22: train loss 1.21508. lr 5.970243e-03: 100%|█| 23/23 [00:07<00:00\n",
      "epoch 10 iter 22: train loss 1.23008. lr 5.963255e-03: 100%|█| 23/23 [00:06<00:0\n",
      "epoch 11 iter 22: train loss 1.22535. lr 5.955536e-03: 100%|█| 23/23 [00:06<00:0\n",
      "epoch 12 iter 22: train loss 1.23645. lr 5.947088e-03: 100%|█| 23/23 [00:06<00:0\n",
      "epoch 13 iter 22: train loss 1.15891. lr 5.937912e-03: 100%|█| 23/23 [00:06<00:0\n",
      "epoch 14 iter 22: train loss 1.22929. lr 5.928010e-03: 100%|█| 23/23 [00:06<00:0\n",
      "epoch 15 iter 22: train loss 1.24636. lr 5.917387e-03: 100%|█| 23/23 [00:06<00:0\n",
      "epoch 16 iter 22: train loss 1.18421. lr 5.906043e-03: 100%|█| 23/23 [00:06<00:0\n",
      "epoch 17 iter 22: train loss 1.23856. lr 5.893982e-03: 100%|█| 23/23 [00:06<00:0\n",
      "epoch 18 iter 22: train loss 1.25678. lr 5.881207e-03: 100%|█| 23/23 [00:06<00:0\n",
      "epoch 19 iter 22: train loss 1.15992. lr 5.867720e-03: 100%|█| 23/23 [00:06<00:0\n",
      "epoch 20 iter 22: train loss 1.15784. lr 5.853526e-03: 100%|█| 23/23 [00:06<00:0\n",
      "epoch 21 iter 22: train loss 1.25902. lr 5.838628e-03: 100%|█| 23/23 [00:06<00:0\n",
      "epoch 22 iter 22: train loss 1.12509. lr 5.823029e-03: 100%|█| 23/23 [00:06<00:0\n",
      "epoch 23 iter 22: train loss 1.07414. lr 5.806733e-03: 100%|█| 23/23 [00:06<00:0\n",
      "epoch 24 iter 22: train loss 1.18026. lr 5.789745e-03: 100%|█| 23/23 [00:06<00:0\n",
      "epoch 25 iter 22: train loss 1.20013. lr 5.772068e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 26 iter 22: train loss 1.06569. lr 5.753707e-03: 100%|█| 23/23 [00:06<00:0\n",
      "epoch 27 iter 22: train loss 1.15519. lr 5.734666e-03: 100%|█| 23/23 [00:06<00:0\n",
      "epoch 28 iter 22: train loss 1.00755. lr 5.714951e-03: 100%|█| 23/23 [00:06<00:0\n",
      "epoch 29 iter 22: train loss 1.05128. lr 5.694565e-03: 100%|█| 23/23 [00:06<00:0\n",
      "epoch 30 iter 22: train loss 1.12254. lr 5.673515e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 31 iter 22: train loss 1.18689. lr 5.651804e-03: 100%|█| 23/23 [00:06<00:0\n",
      "epoch 32 iter 22: train loss 1.10489. lr 5.629439e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 33 iter 22: train loss 1.06820. lr 5.606425e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 34 iter 22: train loss 1.12129. lr 5.582768e-03: 100%|█| 23/23 [00:06<00:0\n",
      "epoch 35 iter 22: train loss 1.02890. lr 5.558474e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 36 iter 22: train loss 1.09428. lr 5.533547e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 37 iter 22: train loss 1.09709. lr 5.507996e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 38 iter 22: train loss 0.93631. lr 5.481826e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 39 iter 22: train loss 1.00120. lr 5.455043e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 40 iter 22: train loss 1.00479. lr 5.427654e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 41 iter 22: train loss 1.03625. lr 5.399666e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 42 iter 22: train loss 0.96027. lr 5.371086e-03: 100%|█| 23/23 [00:06<00:0\n",
      "epoch 43 iter 22: train loss 1.00330. lr 5.341921e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 44 iter 22: train loss 0.95336. lr 5.312178e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 45 iter 22: train loss 0.99625. lr 5.281864e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 46 iter 22: train loss 0.91686. lr 5.250987e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 47 iter 22: train loss 0.99239. lr 5.219554e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 48 iter 22: train loss 0.90472. lr 5.187573e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 49 iter 22: train loss 0.94050. lr 5.155053e-03: 100%|█| 23/23 [00:06<00:0\n",
      "epoch 50 iter 22: train loss 0.90591. lr 5.122001e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 51 iter 22: train loss 0.86471. lr 5.088425e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 52 iter 22: train loss 0.94197. lr 5.054333e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 53 iter 22: train loss 0.88312. lr 5.019735e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 54 iter 22: train loss 0.89714. lr 4.984638e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 55 iter 22: train loss 0.79635. lr 4.949052e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 56 iter 22: train loss 0.93591. lr 4.912984e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 57 iter 22: train loss 0.89493. lr 4.876444e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 58 iter 22: train loss 0.90704. lr 4.839441e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 59 iter 22: train loss 0.90681. lr 4.801984e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 60 iter 22: train loss 0.84466. lr 4.764082e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 61 iter 22: train loss 0.81338. lr 4.725745e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 62 iter 22: train loss 0.82728. lr 4.686982e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 63 iter 22: train loss 0.85133. lr 4.647803e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 64 iter 22: train loss 0.83060. lr 4.608217e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 65 iter 22: train loss 0.82182. lr 4.568234e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 66 iter 22: train loss 0.79854. lr 4.527864e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 67 iter 22: train loss 0.79071. lr 4.487117e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 68 iter 22: train loss 0.75228. lr 4.446003e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 69 iter 22: train loss 0.79474. lr 4.404532e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 70 iter 22: train loss 0.80737. lr 4.362715e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 71 iter 22: train loss 0.76809. lr 4.320561e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 72 iter 22: train loss 0.74892. lr 4.278081e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 73 iter 22: train loss 0.69546. lr 4.235286e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 74 iter 22: train loss 0.76273. lr 4.192186e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 75 iter 22: train loss 0.73232. lr 4.148791e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 76 iter 22: train loss 0.75695. lr 4.105113e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 77 iter 22: train loss 0.79029. lr 4.061163e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 78 iter 22: train loss 0.72922. lr 4.016950e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 79 iter 22: train loss 0.81710. lr 3.972487e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 80 iter 22: train loss 0.66408. lr 3.927783e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 81 iter 22: train loss 0.72479. lr 3.882851e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 82 iter 22: train loss 0.67575. lr 3.837700e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 83 iter 22: train loss 0.67369. lr 3.792343e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 84 iter 22: train loss 0.67718. lr 3.746791e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 85 iter 22: train loss 0.66484. lr 3.701054e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 86 iter 22: train loss 0.67439. lr 3.655144e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 87 iter 22: train loss 0.73015. lr 3.609072e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 88 iter 22: train loss 0.63774. lr 3.562850e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 89 iter 22: train loss 0.64077. lr 3.516489e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 90 iter 22: train loss 0.68091. lr 3.470001e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 91 iter 22: train loss 0.64233. lr 3.423396e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 92 iter 22: train loss 0.69442. lr 3.376687e-03: 100%|█| 23/23 [00:06<00:0\n",
      "epoch 93 iter 22: train loss 0.61709. lr 3.329885e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 94 iter 22: train loss 0.65134. lr 3.283002e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 95 iter 22: train loss 0.62757. lr 3.236049e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 96 iter 22: train loss 0.63570. lr 3.189038e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 97 iter 22: train loss 0.67935. lr 3.141980e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 98 iter 22: train loss 0.60903. lr 3.094886e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 99 iter 22: train loss 0.62178. lr 3.047770e-03: 100%|█| 23/23 [00:07<00:0\n",
      "epoch 100 iter 22: train loss 0.65293. lr 3.000642e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 101 iter 22: train loss 0.59152. lr 2.953513e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 102 iter 22: train loss 0.63599. lr 2.906396e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 103 iter 22: train loss 0.57945. lr 2.859302e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 104 iter 22: train loss 0.57623. lr 2.812243e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 105 iter 22: train loss 0.60771. lr 2.765230e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 106 iter 22: train loss 0.67090. lr 2.718276e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 107 iter 22: train loss 0.63576. lr 2.671390e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 108 iter 22: train loss 0.57594. lr 2.624586e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 109 iter 22: train loss 0.50504. lr 2.577874e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 110 iter 22: train loss 0.53891. lr 2.531267e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 111 iter 22: train loss 0.53234. lr 2.484775e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 112 iter 22: train loss 0.57494. lr 2.438411e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 113 iter 22: train loss 0.55466. lr 2.392185e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 114 iter 22: train loss 0.55248. lr 2.346109e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 115 iter 22: train loss 0.55277. lr 2.300194e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 116 iter 22: train loss 0.53606. lr 2.254452e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 117 iter 22: train loss 0.54683. lr 2.208895e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 118 iter 22: train loss 0.53502. lr 2.163532e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 119 iter 22: train loss 0.50243. lr 2.118376e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 120 iter 22: train loss 0.56017. lr 2.073437e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 121 iter 22: train loss 0.54207. lr 2.028727e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 122 iter 22: train loss 0.51649. lr 1.984257e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 123 iter 22: train loss 0.49447. lr 1.940038e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 124 iter 22: train loss 0.51059. lr 1.896080e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 125 iter 22: train loss 0.52592. lr 1.852394e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 126 iter 22: train loss 0.50394. lr 1.808992e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 127 iter 22: train loss 0.51417. lr 1.765884e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 128 iter 22: train loss 0.53225. lr 1.723080e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 129 iter 22: train loss 0.52926. lr 1.680592e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 130 iter 22: train loss 0.47554. lr 1.638429e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 131 iter 22: train loss 0.49813. lr 1.596602e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 132 iter 22: train loss 0.51351. lr 1.555121e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 133 iter 22: train loss 0.51498. lr 1.513997e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 134 iter 22: train loss 0.52120. lr 1.473240e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 135 iter 22: train loss 0.47003. lr 1.432860e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 136 iter 22: train loss 0.50980. lr 1.392866e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 137 iter 22: train loss 0.47515. lr 1.353269e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 138 iter 22: train loss 0.46901. lr 1.314079e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 139 iter 22: train loss 0.46420. lr 1.275304e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 140 iter 22: train loss 0.50168. lr 1.236956e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 141 iter 22: train loss 0.46879. lr 1.199042e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 142 iter 22: train loss 0.51215. lr 1.161573e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 143 iter 22: train loss 0.45948. lr 1.124557e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 144 iter 22: train loss 0.48692. lr 1.088005e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 145 iter 22: train loss 0.49046. lr 1.051924e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 146 iter 22: train loss 0.47645. lr 1.016324e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 147 iter 22: train loss 0.49784. lr 9.812140e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 148 iter 22: train loss 0.46394. lr 9.466019e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 149 iter 22: train loss 0.46369. lr 9.124966e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 150 iter 22: train loss 0.48318. lr 8.789065e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 151 iter 22: train loss 0.45699. lr 8.458399e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 152 iter 22: train loss 0.46247. lr 8.133050e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 153 iter 22: train loss 0.50657. lr 7.813097e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 154 iter 22: train loss 0.47292. lr 7.498620e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 155 iter 22: train loss 0.48859. lr 7.189696e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 156 iter 22: train loss 0.45784. lr 6.886402e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 157 iter 22: train loss 0.43243. lr 6.588813e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 158 iter 22: train loss 0.47785. lr 6.297002e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 159 iter 22: train loss 0.45917. lr 6.011040e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 160 iter 22: train loss 0.45888. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 161 iter 22: train loss 0.47449. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 162 iter 22: train loss 0.44878. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 163 iter 22: train loss 0.41681. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 164 iter 22: train loss 0.46897. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 165 iter 22: train loss 0.42172. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 166 iter 22: train loss 0.45609. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 167 iter 22: train loss 0.44669. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 168 iter 22: train loss 0.46356. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 169 iter 22: train loss 0.45990. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 170 iter 22: train loss 0.45803. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 171 iter 22: train loss 0.45519. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 172 iter 22: train loss 0.44058. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 173 iter 22: train loss 0.45027. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 174 iter 22: train loss 0.46286. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 175 iter 22: train loss 0.44799. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 176 iter 22: train loss 0.44546. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 177 iter 22: train loss 0.46776. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 178 iter 22: train loss 0.41024. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 179 iter 22: train loss 0.42551. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 180 iter 22: train loss 0.44163. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 181 iter 22: train loss 0.43976. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 182 iter 22: train loss 0.40114. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 183 iter 22: train loss 0.42983. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 184 iter 22: train loss 0.43403. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 185 iter 22: train loss 0.45935. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 186 iter 22: train loss 0.44709. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 187 iter 22: train loss 0.42324. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 188 iter 22: train loss 0.41969. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 189 iter 22: train loss 0.42487. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 190 iter 22: train loss 0.42875. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 191 iter 22: train loss 0.45072. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 192 iter 22: train loss 0.42068. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 193 iter 22: train loss 0.44921. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 194 iter 22: train loss 0.41376. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 195 iter 22: train loss 0.41351. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 196 iter 22: train loss 0.43190. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 197 iter 22: train loss 0.41982. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 198 iter 22: train loss 0.42040. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 199 iter 22: train loss 0.42930. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 200 iter 22: train loss 0.45922. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 201 iter 22: train loss 0.43362. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 202 iter 22: train loss 0.40368. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 203 iter 22: train loss 0.41406. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 204 iter 22: train loss 0.42452. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 205 iter 22: train loss 0.40200. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 206 iter 22: train loss 0.41763. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 207 iter 22: train loss 0.44329. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 208 iter 22: train loss 0.44786. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 209 iter 22: train loss 0.39906. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 210 iter 22: train loss 0.41518. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 211 iter 22: train loss 0.40804. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 212 iter 22: train loss 0.42999. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 213 iter 22: train loss 0.43965. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 214 iter 22: train loss 0.41138. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 215 iter 22: train loss 0.41585. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 216 iter 22: train loss 0.42652. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 217 iter 22: train loss 0.40455. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 218 iter 22: train loss 0.40954. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 219 iter 22: train loss 0.41870. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 220 iter 22: train loss 0.44278. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 221 iter 22: train loss 0.42952. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 222 iter 22: train loss 0.41804. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 223 iter 22: train loss 0.42193. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 224 iter 22: train loss 0.39567. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 225 iter 22: train loss 0.43417. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 226 iter 22: train loss 0.39375. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 227 iter 22: train loss 0.40419. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 228 iter 22: train loss 0.39283. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 229 iter 22: train loss 0.40122. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 230 iter 22: train loss 0.40742. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 231 iter 22: train loss 0.41451. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 232 iter 22: train loss 0.44146. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 233 iter 22: train loss 0.43068. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 234 iter 22: train loss 0.42326. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 235 iter 22: train loss 0.39618. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 236 iter 22: train loss 0.44430. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 237 iter 22: train loss 0.40695. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 238 iter 22: train loss 0.38668. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 239 iter 22: train loss 0.40093. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 240 iter 22: train loss 0.42455. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 241 iter 22: train loss 0.43800. lr 6.011040e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 242 iter 22: train loss 0.40187. lr 6.297002e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 243 iter 22: train loss 0.41160. lr 6.588813e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 244 iter 22: train loss 0.41894. lr 6.886402e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 245 iter 22: train loss 0.41563. lr 7.189696e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 246 iter 22: train loss 0.41868. lr 7.498620e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 247 iter 22: train loss 0.41299. lr 7.813097e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 248 iter 22: train loss 0.41482. lr 8.133050e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 249 iter 22: train loss 0.40285. lr 8.458399e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 250 iter 22: train loss 0.41387. lr 8.789065e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 251 iter 22: train loss 0.40422. lr 9.124966e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 252 iter 22: train loss 0.37865. lr 9.466019e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 253 iter 22: train loss 0.42844. lr 9.812140e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 254 iter 22: train loss 0.41169. lr 1.016324e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 255 iter 22: train loss 0.42384. lr 1.051924e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 256 iter 22: train loss 0.40613. lr 1.088005e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 257 iter 22: train loss 0.39246. lr 1.124557e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 258 iter 22: train loss 0.40534. lr 1.161573e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 259 iter 22: train loss 0.43974. lr 1.199042e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 260 iter 22: train loss 0.39857. lr 1.236956e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 261 iter 22: train loss 0.42306. lr 1.275304e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 262 iter 22: train loss 0.43593. lr 1.314079e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 263 iter 22: train loss 0.42294. lr 1.353269e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 264 iter 22: train loss 0.41004. lr 1.392866e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 265 iter 22: train loss 0.40666. lr 1.432860e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 266 iter 22: train loss 0.46091. lr 1.473240e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 267 iter 22: train loss 0.39990. lr 1.513997e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 268 iter 22: train loss 0.45112. lr 1.555121e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 269 iter 22: train loss 0.44090. lr 1.596602e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 270 iter 22: train loss 0.39148. lr 1.638429e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 271 iter 22: train loss 0.39165. lr 1.680592e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 272 iter 22: train loss 0.41758. lr 1.723080e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 273 iter 22: train loss 0.41686. lr 1.765884e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 274 iter 22: train loss 0.39875. lr 1.808992e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 275 iter 22: train loss 0.43941. lr 1.852394e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 276 iter 22: train loss 0.45386. lr 1.896080e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 277 iter 22: train loss 0.43202. lr 1.940038e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 278 iter 22: train loss 0.41326. lr 1.984257e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 279 iter 22: train loss 0.41398. lr 2.028727e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 280 iter 22: train loss 0.43293. lr 2.073437e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 281 iter 22: train loss 0.45310. lr 2.118376e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 282 iter 22: train loss 0.44088. lr 2.163532e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 283 iter 22: train loss 0.42974. lr 2.208895e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 284 iter 22: train loss 0.44735. lr 2.254452e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 285 iter 22: train loss 0.42021. lr 2.300194e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 286 iter 22: train loss 0.45608. lr 2.346109e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 287 iter 22: train loss 0.40142. lr 2.392185e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 288 iter 22: train loss 0.45472. lr 2.438411e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 289 iter 22: train loss 0.44307. lr 2.484775e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 290 iter 22: train loss 0.43149. lr 2.531267e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 291 iter 22: train loss 0.39904. lr 2.577874e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 292 iter 22: train loss 0.42685. lr 2.624586e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 293 iter 22: train loss 0.46709. lr 2.671390e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 294 iter 22: train loss 0.42867. lr 2.718276e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 295 iter 22: train loss 0.42844. lr 2.765230e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 296 iter 22: train loss 0.46757. lr 2.812243e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 297 iter 22: train loss 0.43539. lr 2.859302e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 298 iter 22: train loss 0.45892. lr 2.906396e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 299 iter 22: train loss 0.44813. lr 2.953513e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 300 iter 22: train loss 0.44631. lr 3.000642e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 301 iter 22: train loss 0.42896. lr 3.047770e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 302 iter 22: train loss 0.44352. lr 3.094886e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 303 iter 22: train loss 0.44263. lr 3.141980e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 304 iter 22: train loss 0.42921. lr 3.189038e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 305 iter 22: train loss 0.46985. lr 3.236049e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 306 iter 22: train loss 0.43979. lr 3.283002e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 307 iter 22: train loss 0.43038. lr 3.329885e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 308 iter 22: train loss 0.45219. lr 3.376687e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 309 iter 22: train loss 0.48540. lr 3.423396e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 310 iter 22: train loss 0.42504. lr 3.470001e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 311 iter 22: train loss 0.43851. lr 3.516489e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 312 iter 22: train loss 0.49004. lr 3.562850e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 313 iter 22: train loss 0.45060. lr 3.609072e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 314 iter 22: train loss 0.47893. lr 3.655144e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 315 iter 22: train loss 0.44860. lr 3.701054e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 316 iter 22: train loss 0.47094. lr 3.746791e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 317 iter 22: train loss 0.42023. lr 3.792343e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 318 iter 22: train loss 0.45185. lr 3.837700e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 319 iter 22: train loss 0.46591. lr 3.882851e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 320 iter 22: train loss 0.47003. lr 3.927783e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 321 iter 22: train loss 0.45053. lr 3.972487e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 322 iter 22: train loss 0.45734. lr 4.016950e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 323 iter 22: train loss 0.44682. lr 4.061163e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 324 iter 22: train loss 0.47380. lr 4.105113e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 325 iter 22: train loss 0.44580. lr 4.148791e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 326 iter 22: train loss 0.46455. lr 4.192186e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 327 iter 22: train loss 0.45635. lr 4.235286e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 328 iter 22: train loss 0.45894. lr 4.278081e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 329 iter 22: train loss 0.47455. lr 4.320561e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 330 iter 22: train loss 0.50270. lr 4.362715e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 331 iter 22: train loss 0.47249. lr 4.404532e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 332 iter 22: train loss 0.44671. lr 4.446003e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 333 iter 22: train loss 0.44374. lr 4.487117e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 334 iter 22: train loss 0.45153. lr 4.527864e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 335 iter 22: train loss 0.48970. lr 4.568234e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 336 iter 22: train loss 0.48854. lr 4.608217e-03: 100%|█| 23/23 [00:06<00:\n",
      "epoch 337 iter 22: train loss 0.45317. lr 4.647803e-03: 100%|█| 23/23 [00:06<00:\n",
      "epoch 338 iter 22: train loss 0.44593. lr 4.686982e-03: 100%|█| 23/23 [00:06<00:\n",
      "epoch 339 iter 22: train loss 0.43653. lr 4.725745e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 340 iter 22: train loss 0.46305. lr 4.764082e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 341 iter 22: train loss 0.45041. lr 4.801984e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 342 iter 22: train loss 0.45382. lr 4.839441e-03: 100%|█| 23/23 [00:06<00:\n",
      "epoch 343 iter 22: train loss 0.45294. lr 4.876444e-03: 100%|█| 23/23 [00:06<00:\n",
      "epoch 344 iter 22: train loss 0.46380. lr 4.912984e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 345 iter 22: train loss 0.41587. lr 4.949052e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 346 iter 22: train loss 0.46137. lr 4.984638e-03: 100%|█| 23/23 [00:06<00:\n",
      "epoch 347 iter 22: train loss 0.44020. lr 5.019735e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 348 iter 22: train loss 0.48808. lr 5.054333e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 349 iter 22: train loss 0.45941. lr 5.088425e-03: 100%|█| 23/23 [00:06<00:\n",
      "epoch 350 iter 22: train loss 0.48212. lr 5.122001e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 351 iter 22: train loss 0.44343. lr 5.155053e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 352 iter 22: train loss 0.46822. lr 5.187573e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 353 iter 22: train loss 0.46585. lr 5.219554e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 354 iter 22: train loss 0.45133. lr 5.250987e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 355 iter 22: train loss 0.45230. lr 5.281864e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 356 iter 22: train loss 0.49161. lr 5.312178e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 357 iter 22: train loss 0.43294. lr 5.341921e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 358 iter 22: train loss 0.46468. lr 5.371086e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 359 iter 22: train loss 0.47098. lr 5.399666e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 360 iter 22: train loss 0.47091. lr 5.427654e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 361 iter 22: train loss 0.46329. lr 5.455043e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 362 iter 22: train loss 0.49055. lr 5.481826e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 363 iter 22: train loss 0.45779. lr 5.507996e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 364 iter 22: train loss 0.46464. lr 5.533547e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 365 iter 22: train loss 0.45301. lr 5.558474e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 366 iter 22: train loss 0.45665. lr 5.582768e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 367 iter 22: train loss 0.42963. lr 5.606425e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 368 iter 22: train loss 0.46332. lr 5.629439e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 369 iter 22: train loss 0.45639. lr 5.651804e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 370 iter 22: train loss 0.41937. lr 5.673515e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 371 iter 22: train loss 0.42427. lr 5.694565e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 372 iter 22: train loss 0.44081. lr 5.714951e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 373 iter 22: train loss 0.47077. lr 5.734666e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 374 iter 22: train loss 0.40482. lr 5.753707e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 375 iter 22: train loss 0.48409. lr 5.772068e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 376 iter 22: train loss 0.44582. lr 5.789745e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 377 iter 22: train loss 0.43217. lr 5.806733e-03: 100%|█| 23/23 [00:06<00:\n",
      "epoch 378 iter 22: train loss 0.43199. lr 5.823029e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 379 iter 22: train loss 0.42317. lr 5.838628e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 380 iter 22: train loss 0.41762. lr 5.853526e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 381 iter 22: train loss 0.44273. lr 5.867720e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 382 iter 22: train loss 0.45680. lr 5.881207e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 383 iter 22: train loss 0.43854. lr 5.893982e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 384 iter 22: train loss 0.47988. lr 5.906043e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 385 iter 22: train loss 0.47119. lr 5.917387e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 386 iter 22: train loss 0.45646. lr 5.928010e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 387 iter 22: train loss 0.44395. lr 5.937912e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 388 iter 22: train loss 0.43649. lr 5.947088e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 389 iter 22: train loss 0.39929. lr 5.955536e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 390 iter 22: train loss 0.42850. lr 5.963255e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 391 iter 22: train loss 0.43488. lr 5.970243e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 392 iter 22: train loss 0.40131. lr 5.976498e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 393 iter 22: train loss 0.47477. lr 5.982019e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 394 iter 22: train loss 0.40938. lr 5.986803e-03: 100%|█| 23/23 [00:06<00:\n",
      "epoch 395 iter 22: train loss 0.46149. lr 5.990850e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 396 iter 22: train loss 0.41174. lr 5.994159e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 397 iter 22: train loss 0.43636. lr 5.996729e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 398 iter 22: train loss 0.42920. lr 5.998559e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 399 iter 22: train loss 0.42791. lr 5.999650e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 400 iter 22: train loss 0.41959. lr 6.000000e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 401 iter 22: train loss 0.43803. lr 5.999609e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 402 iter 22: train loss 0.42100. lr 5.998479e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 403 iter 22: train loss 0.44572. lr 5.996608e-03: 100%|█| 23/23 [00:06<00:\n",
      "epoch 404 iter 22: train loss 0.42008. lr 5.993998e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 405 iter 22: train loss 0.45974. lr 5.990649e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 406 iter 22: train loss 0.39842. lr 5.986561e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 407 iter 22: train loss 0.45186. lr 5.981737e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 408 iter 22: train loss 0.42000. lr 5.976177e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 409 iter 22: train loss 0.44519. lr 5.969882e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 410 iter 22: train loss 0.41740. lr 5.962854e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 411 iter 22: train loss 0.43851. lr 5.955095e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 412 iter 22: train loss 0.43629. lr 5.946607e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 413 iter 22: train loss 0.42899. lr 5.937391e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 414 iter 22: train loss 0.38759. lr 5.927450e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 415 iter 22: train loss 0.41867. lr 5.916787e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 416 iter 22: train loss 0.41231. lr 5.905404e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 417 iter 22: train loss 0.44199. lr 5.893305e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 418 iter 22: train loss 0.42858. lr 5.880490e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 419 iter 22: train loss 0.38216. lr 5.866966e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 420 iter 22: train loss 0.43656. lr 5.852733e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 421 iter 22: train loss 0.41863. lr 5.837796e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 422 iter 22: train loss 0.41758. lr 5.822159e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 423 iter 22: train loss 0.44904. lr 5.805826e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 424 iter 22: train loss 0.39907. lr 5.788800e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 425 iter 22: train loss 0.43158. lr 5.771086e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 426 iter 22: train loss 0.39944. lr 5.752688e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 427 iter 22: train loss 0.41804. lr 5.733610e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 428 iter 22: train loss 0.45834. lr 5.713858e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 429 iter 22: train loss 0.36389. lr 5.693436e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 430 iter 22: train loss 0.38999. lr 5.672349e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 431 iter 22: train loss 0.41333. lr 5.650603e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 432 iter 22: train loss 0.38456. lr 5.628203e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 433 iter 22: train loss 0.40569. lr 5.605153e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 434 iter 22: train loss 0.39905. lr 5.581461e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 435 iter 22: train loss 0.39980. lr 5.557132e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 436 iter 22: train loss 0.41753. lr 5.532172e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 437 iter 22: train loss 0.41952. lr 5.506587e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 438 iter 22: train loss 0.39507. lr 5.480383e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 439 iter 22: train loss 0.40357. lr 5.453567e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 440 iter 22: train loss 0.37484. lr 5.426145e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 441 iter 22: train loss 0.36967. lr 5.398125e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 442 iter 22: train loss 0.38220. lr 5.369513e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 443 iter 22: train loss 0.40475. lr 5.340316e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 444 iter 22: train loss 0.41231. lr 5.310541e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 445 iter 22: train loss 0.40639. lr 5.280197e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 446 iter 22: train loss 0.36330. lr 5.249289e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 447 iter 22: train loss 0.36961. lr 5.217826e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 448 iter 22: train loss 0.40500. lr 5.185816e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 449 iter 22: train loss 0.37847. lr 5.153267e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 450 iter 22: train loss 0.36836. lr 5.120186e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 451 iter 22: train loss 0.40875. lr 5.086581e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 452 iter 22: train loss 0.40267. lr 5.052462e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 453 iter 22: train loss 0.39883. lr 5.017837e-03: 100%|█| 23/23 [00:06<00:\n",
      "epoch 454 iter 22: train loss 0.37832. lr 4.982713e-03: 100%|█| 23/23 [00:06<00:\n",
      "epoch 455 iter 22: train loss 0.37866. lr 4.947100e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 456 iter 22: train loss 0.38584. lr 4.911006e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 457 iter 22: train loss 0.38102. lr 4.874441e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 458 iter 22: train loss 0.37312. lr 4.837413e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 459 iter 22: train loss 0.36824. lr 4.799931e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 460 iter 22: train loss 0.38757. lr 4.762006e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 461 iter 22: train loss 0.37435. lr 4.723645e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 462 iter 22: train loss 0.38819. lr 4.684859e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 463 iter 22: train loss 0.38468. lr 4.645658e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 464 iter 22: train loss 0.37358. lr 4.606050e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 465 iter 22: train loss 0.37935. lr 4.566046e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 466 iter 22: train loss 0.36420. lr 4.525655e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 467 iter 22: train loss 0.36905. lr 4.484888e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 468 iter 22: train loss 0.34489. lr 4.443754e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 469 iter 22: train loss 0.38074. lr 4.402264e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 470 iter 22: train loss 0.36945. lr 4.360428e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 471 iter 22: train loss 0.36362. lr 4.318256e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 472 iter 22: train loss 0.36467. lr 4.275758e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 473 iter 22: train loss 0.34844. lr 4.232946e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 474 iter 22: train loss 0.34565. lr 4.189830e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 475 iter 22: train loss 0.35242. lr 4.146420e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 476 iter 22: train loss 0.38323. lr 4.102727e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 477 iter 22: train loss 0.35868. lr 4.058762e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 478 iter 22: train loss 0.37279. lr 4.014535e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 479 iter 22: train loss 0.33387. lr 3.970058e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 480 iter 22: train loss 0.33200. lr 3.925342e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 481 iter 22: train loss 0.33350. lr 3.880397e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 482 iter 22: train loss 0.36219. lr 3.835236e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 483 iter 22: train loss 0.35522. lr 3.789867e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 484 iter 22: train loss 0.33909. lr 3.744304e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 485 iter 22: train loss 0.35867. lr 3.698558e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 486 iter 22: train loss 0.34938. lr 3.652639e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 487 iter 22: train loss 0.31477. lr 3.606558e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 488 iter 22: train loss 0.32360. lr 3.560329e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 489 iter 22: train loss 0.35261. lr 3.513960e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 490 iter 22: train loss 0.35357. lr 3.467465e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 491 iter 22: train loss 0.32001. lr 3.420855e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 492 iter 22: train loss 0.34235. lr 3.374141e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 493 iter 22: train loss 0.33967. lr 3.327334e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 494 iter 22: train loss 0.33267. lr 3.280447e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 495 iter 22: train loss 0.33299. lr 3.233490e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 496 iter 22: train loss 0.32006. lr 3.186476e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 497 iter 22: train loss 0.32500. lr 3.139416e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 498 iter 22: train loss 0.34341. lr 3.092321e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 499 iter 22: train loss 0.31367. lr 3.045204e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 500 iter 22: train loss 0.32472. lr 2.998075e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 501 iter 22: train loss 0.32679. lr 2.950947e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 502 iter 22: train loss 0.32734. lr 2.903831e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 503 iter 22: train loss 0.33186. lr 2.856739e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 504 iter 22: train loss 0.33650. lr 2.809682e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 505 iter 22: train loss 0.31077. lr 2.762672e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 506 iter 22: train loss 0.33095. lr 2.715720e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 507 iter 22: train loss 0.30710. lr 2.668839e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 508 iter 22: train loss 0.34138. lr 2.622040e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 509 iter 22: train loss 0.30746. lr 2.575333e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 510 iter 22: train loss 0.31778. lr 2.528732e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 511 iter 22: train loss 0.29318. lr 2.482247e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 512 iter 22: train loss 0.27661. lr 2.435890e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 513 iter 22: train loss 0.29615. lr 2.389672e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 514 iter 22: train loss 0.32577. lr 2.343604e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 515 iter 22: train loss 0.28243. lr 2.297699e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 516 iter 22: train loss 0.32519. lr 2.251967e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 517 iter 22: train loss 0.28882. lr 2.206419e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 518 iter 22: train loss 0.31695. lr 2.161067e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 519 iter 22: train loss 0.32116. lr 2.115923e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 520 iter 22: train loss 0.28329. lr 2.070996e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 521 iter 22: train loss 0.29743. lr 2.026299e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 522 iter 22: train loss 0.27559. lr 1.981842e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 523 iter 22: train loss 0.27977. lr 1.937637e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 524 iter 22: train loss 0.30846. lr 1.893694e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 525 iter 22: train loss 0.28281. lr 1.850023e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 526 iter 22: train loss 0.29297. lr 1.806637e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 527 iter 22: train loss 0.27307. lr 1.763545e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 528 iter 22: train loss 0.28593. lr 1.720758e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 529 iter 22: train loss 0.28256. lr 1.678287e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 530 iter 22: train loss 0.28784. lr 1.636142e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 531 iter 22: train loss 0.30002. lr 1.594334e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 532 iter 22: train loss 0.28684. lr 1.552873e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 533 iter 22: train loss 0.25993. lr 1.511768e-03: 100%|█| 23/23 [00:08<00:\n",
      "epoch 534 iter 22: train loss 0.27938. lr 1.471031e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 535 iter 22: train loss 0.27407. lr 1.430672e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 536 iter 22: train loss 0.29378. lr 1.390700e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 537 iter 22: train loss 0.27074. lr 1.351125e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 538 iter 22: train loss 0.27643. lr 1.311957e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 539 iter 22: train loss 0.27576. lr 1.273205e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 540 iter 22: train loss 0.26987. lr 1.234880e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 541 iter 22: train loss 0.26387. lr 1.196990e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 542 iter 22: train loss 0.25338. lr 1.159545e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 543 iter 22: train loss 0.25939. lr 1.122555e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 544 iter 22: train loss 0.27487. lr 1.086028e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 545 iter 22: train loss 0.25810. lr 1.049973e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 546 iter 22: train loss 0.25345. lr 1.014400e-03: 100%|█| 23/23 [00:07<00:\n",
      "epoch 547 iter 22: train loss 0.25977. lr 9.793162e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 548 iter 22: train loss 0.25146. lr 9.447315e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 549 iter 22: train loss 0.25813. lr 9.106540e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 550 iter 22: train loss 0.26453. lr 8.770922e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 551 iter 22: train loss 0.25223. lr 8.440543e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 552 iter 22: train loss 0.26798. lr 8.115486e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 553 iter 22: train loss 0.26440. lr 7.795829e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 554 iter 22: train loss 0.25722. lr 7.481653e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 555 iter 22: train loss 0.26801. lr 7.173034e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 556 iter 22: train loss 0.25532. lr 6.870049e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 557 iter 22: train loss 0.25668. lr 6.572772e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 558 iter 22: train loss 0.24580. lr 6.281277e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 559 iter 22: train loss 0.25335. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 560 iter 22: train loss 0.27065. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 561 iter 22: train loss 0.25797. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 562 iter 22: train loss 0.25035. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 563 iter 22: train loss 0.26738. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 564 iter 22: train loss 0.24641. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 565 iter 22: train loss 0.26778. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 566 iter 22: train loss 0.25695. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 567 iter 22: train loss 0.24392. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 568 iter 22: train loss 0.24656. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 569 iter 22: train loss 0.25977. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 570 iter 22: train loss 0.26889. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 571 iter 22: train loss 0.23667. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 572 iter 22: train loss 0.25372. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 573 iter 22: train loss 0.23667. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 574 iter 22: train loss 0.23729. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 575 iter 22: train loss 0.25141. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 576 iter 22: train loss 0.24562. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 577 iter 22: train loss 0.24653. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 578 iter 22: train loss 0.23025. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 579 iter 22: train loss 0.26283. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 580 iter 22: train loss 0.23825. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 581 iter 22: train loss 0.25321. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 582 iter 22: train loss 0.23990. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 583 iter 22: train loss 0.24878. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 584 iter 22: train loss 0.25395. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 585 iter 22: train loss 0.25219. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 586 iter 22: train loss 0.25342. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 587 iter 22: train loss 0.25352. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 588 iter 22: train loss 0.23907. lr 6.000000e-04: 100%|█| 23/23 [00:06<00:\n",
      "epoch 589 iter 22: train loss 0.24361. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 590 iter 22: train loss 0.25779. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 591 iter 22: train loss 0.24239. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 592 iter 22: train loss 0.26294. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 593 iter 22: train loss 0.24252. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 594 iter 22: train loss 0.22283. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 595 iter 22: train loss 0.22813. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 596 iter 22: train loss 0.24785. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 597 iter 22: train loss 0.24206. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 598 iter 22: train loss 0.24655. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 599 iter 22: train loss 0.23081. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 600 iter 22: train loss 0.23536. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 601 iter 22: train loss 0.25220. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 602 iter 22: train loss 0.24147. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 603 iter 22: train loss 0.24699. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 604 iter 22: train loss 0.23026. lr 6.000000e-04: 100%|█| 23/23 [00:06<00:\n",
      "epoch 605 iter 22: train loss 0.25367. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 606 iter 22: train loss 0.23968. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 607 iter 22: train loss 0.23932. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 608 iter 22: train loss 0.24396. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 609 iter 22: train loss 0.24327. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 610 iter 22: train loss 0.25153. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 611 iter 22: train loss 0.23396. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 612 iter 22: train loss 0.23838. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 613 iter 22: train loss 0.24624. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 614 iter 22: train loss 0.24389. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 615 iter 22: train loss 0.24055. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 616 iter 22: train loss 0.24785. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 617 iter 22: train loss 0.24035. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 618 iter 22: train loss 0.25046. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 619 iter 22: train loss 0.24856. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 620 iter 22: train loss 0.26568. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 621 iter 22: train loss 0.23749. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 622 iter 22: train loss 0.25338. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 623 iter 22: train loss 0.24112. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 624 iter 22: train loss 0.23365. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 625 iter 22: train loss 0.24801. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 626 iter 22: train loss 0.24175. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 627 iter 22: train loss 0.24385. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 628 iter 22: train loss 0.25254. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 629 iter 22: train loss 0.24077. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 630 iter 22: train loss 0.24685. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 631 iter 22: train loss 0.23631. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 632 iter 22: train loss 0.23857. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 633 iter 22: train loss 0.24730. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 634 iter 22: train loss 0.25261. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 635 iter 22: train loss 0.23293. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 636 iter 22: train loss 0.22612. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 637 iter 22: train loss 0.24019. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 638 iter 22: train loss 0.24549. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 639 iter 22: train loss 0.24410. lr 6.000000e-04: 100%|█| 23/23 [00:06<00:\n",
      "epoch 640 iter 22: train loss 0.24329. lr 6.000000e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 641 iter 22: train loss 0.24426. lr 6.026461e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 642 iter 22: train loss 0.25902. lr 6.312743e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 643 iter 22: train loss 0.24196. lr 6.604871e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 644 iter 22: train loss 0.24051. lr 6.902773e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 645 iter 22: train loss 0.24128. lr 7.206376e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 646 iter 22: train loss 0.25389. lr 7.515604e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 647 iter 22: train loss 0.25543. lr 7.830381e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 648 iter 22: train loss 0.25307. lr 8.150630e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 649 iter 22: train loss 0.24039. lr 8.476271e-04: 100%|█| 23/23 [00:07<00:\n",
      "epoch 650 iter 22: train loss 0.24899. lr 8.807224e-04: 100%|█| 23/23 [00:07<00:\n"
     ]
    }
   ],
   "source": [
    "# Pretrain the model\n",
    "! python src/run.py pretrain vanilla wiki.txt \\\n",
    "--writing_params_path vanilla.pretrain.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "989fd4b8-6c13-47df-9e49-78228791bbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 418352 characters, 256 unique.\n",
      "epoch 1 iter 7: train loss 0.10910. lr 5.999844e-04: 100%|█| 8/8 [00:04<00:00,  \n",
      "epoch 2 iter 7: train loss 0.05838. lr 5.999351e-04: 100%|█| 8/8 [00:04<00:00,  \n",
      "epoch 3 iter 7: train loss 0.05084. lr 5.998521e-04: 100%|█| 8/8 [00:04<00:00,  \n",
      "epoch 4 iter 7: train loss 0.04648. lr 5.997352e-04: 100%|█| 8/8 [00:04<00:00,  \n",
      "epoch 5 iter 7: train loss 0.04344. lr 5.995847e-04: 100%|█| 8/8 [00:04<00:00,  \n",
      "epoch 6 iter 7: train loss 0.04039. lr 5.994004e-04: 100%|█| 8/8 [00:04<00:00,  \n",
      "epoch 7 iter 7: train loss 0.03772. lr 5.991823e-04: 100%|█| 8/8 [00:04<00:00,  \n",
      "epoch 8 iter 7: train loss 0.03528. lr 5.989306e-04: 100%|█| 8/8 [00:04<00:00,  \n",
      "epoch 9 iter 7: train loss 0.03078. lr 5.986453e-04: 100%|█| 8/8 [00:04<00:00,  \n",
      "epoch 10 iter 7: train loss 0.02831. lr 5.983263e-04: 100%|█| 8/8 [00:04<00:00, \n"
     ]
    }
   ],
   "source": [
    "# Finetune the model\n",
    "!python src/run.py finetune vanilla wiki.txt \\\n",
    "--reading_params_path vanilla.pretrain.params \\\n",
    "--writing_params_path vanilla.finetune.params \\\n",
    "--finetune_corpus_path birth_places_train.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44351091-69ca-4ba8-8787-b2d603cdc43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 418352 characters, 256 unique.\n",
      "500it [00:32, 15.35it/s]\n",
      "Correct: 78.0 out of 500.0: 15.6%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the dev set; write to disk\n",
    "!python src/run.py evaluate vanilla wiki.txt \\\n",
    "--reading_params_path vanilla.finetune.params \\\n",
    "--eval_corpus_path birth_dev.tsv \\\n",
    "--outputs_path vanilla.pretrain.dev.predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc1a44a-f71c-4db5-b7f9-3c1018b97e79",
   "metadata": {},
   "source": [
    "### g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a30db2-f668-47ae-a310-3f4a16651073",
   "metadata": {},
   "source": [
    "As was discussed, the regular Transformer scales quadratically with the number of tokens L\n",
    "in the input sequence, which is prohibitively expensive for large L and precludes its usage in\n",
    "settings with limited computational resources even for moderate values of L. At the lecture\n",
    "we discussed two methods that approached this problem, i.e Linformer and Big Bird models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d3167e-8d9f-470a-a924-1e2d9279f60e",
   "metadata": {},
   "source": [
    "#### Question 1: Explain in no more than 3 sentences the idea behind Linformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d5be5c-2494-4582-83f5-c63b68dced2c",
   "metadata": {},
   "source": [
    "> The linformer is based on the idea of reducing the key and value dimension via a projection to a minor dimension with a lower-rank matrix. The authors suggest that a context mapping matrix is of a low order, so it is possible to modify this matrix to reduce the number of operations and memory needed, to compute each individual head of attention, from square to linear, with respect to the length of the sequence under consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e7163e-f37c-4a54-a1f2-f0e983c1e23e",
   "metadata": {},
   "source": [
    "#### Question 2: Explain in no more than 3 sentences the idea behind BigBird"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb69f4bb-2213-4a6d-85cc-79088409ea1b",
   "metadata": {},
   "source": [
    "> In the approach presented in Big Bird, the authors use sprase attention matrices (instead of the original full matrices), which consists of 3 three blocks: random attention, window attention, global attention. The authors show that the resulting sparse attention mechanism is equivalent to full attetion mechanism, if the number of attention layers is increased. The number of operations and memories needed is reduced from quadratic to linear, since said blocks depend on predetermined parameters and not on the length of the processed sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807f7304-7409-407d-ab70-ee4a1379b2bd",
   "metadata": {},
   "source": [
    "#### Question 3: In what respects BigBird is as powerful and expressive as full-attention mechanisms? Explain in up to 3 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25ac42f-728e-4657-99fb-74b5ff0da3cc",
   "metadata": {},
   "source": [
    "> Authors showed that the sparse attention mechanism defined by any graph containing the star-graph is a universal approximator of sequence functions. They also showed that they can use a sparse encoder and sparse decoder to simulate any Turing Machine; these two features are also features of the full-attention mechanism. Also their empirically results are also eligible, based on benchmarks, they represent the effectiveness of their approach comparable to, and sometimes even greater than, the original attention mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ad56da-0cf5-422d-bfd9-bc5aff421de5",
   "metadata": {},
   "source": [
    "### h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f306f98c-a8da-4d6f-948d-b53ea4c39c1b",
   "metadata": {},
   "source": [
    "#### Question 1: Explain in less than 3 sentences high-level idea of attention approximation from the paper to speed up computations? What are expected speedups?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c671ac-d26f-4654-9854-24b3f83ca0f5",
   "metadata": {},
   "source": [
    "> Authors present empirically and theoretically, that the FAVOR + mechanism, which is intended to approximate the attention matrix and decomposition (which is used to change the order of matrix multiplication), is characterized by space complexity $O(Lr + Ld + rd)$ and time complexity $O(Lrd)$ as opposed to $O(L^2 + Ld)$ and $O(L^2d)$ of the regular attention, where $L$ is the length of the sequence, $d$ is the dimension of the latent space, and $r$ it is dimensionality of feature map codomain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221949a0-d055-4906-8923-a677ee93f31d",
   "metadata": {},
   "source": [
    "#### Question 2: What problem might arise when the the softmax-kernel which defines regular attention matrix is approximated naively by trigonometric functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6935eb73-3b7c-4a24-a7a8-0327524509dc",
   "metadata": {},
   "source": [
    ">> \"Applying random feature maps with potentially negative dimension-values (sin / cos) leads to unstable behaviours, especially when kernel scores close to 0.. are approximated by estimators with large variance in such regions.\"\n",
    "\n",
    ">Authors provide detailed theoretical explanations showing that the variance of the estimator of such an approximated regular attention matrix\n",
    "is large as approximated values tend to 0, and show empirically that it is useful to use kernels producing non-negative scores, because in this case it results in negative-diagonal-values of renormalizers, and consequently either completely prevents training or leads to sub-optimal models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4570e40-f5ed-4387-af24-05c31b9b0be6",
   "metadata": {},
   "source": [
    "#### Question 3: How does authors of the paper avoid the problem from question 2? What approximation can help here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810fdbdb-b75a-4fb2-b073-9c81fda08e47",
   "metadata": {},
   "source": [
    "The authors propose an unbiased estimator that is based on the positive random feature map : $\\phi(x) = exp(\\omega^T\\cdot x-\\frac{||x||^2}{2})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d357d87-63a6-460e-a023-fffe31605882",
   "metadata": {},
   "source": [
    "#### Question 4: What are we achieving by sampling orthogonal random features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced3bb20-ffb0-4cc0-aed9-b84be3703d05",
   "metadata": {},
   "source": [
    "In this way, we can reduce the variance of the estimator even further, for any dimensionality $d$ rather than just asymptotically for large enough $d$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71c20fc-760f-4660-ab57-f0168e9c2a95",
   "metadata": {},
   "source": [
    "# Section 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ca8f58-6a14-413b-95bc-e7ba577110d1",
   "metadata": {},
   "source": [
    "### a)\n",
    "\n",
    "#### Briefly explain why the pretrained (vanilla) model was able to achieve an accuracy of above 10%, whereas the non-pretrained model was not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60b5d3c-d1cb-46f3-a2de-a864db6dd4f6",
   "metadata": {},
   "source": [
    "> During the pretraining phase, the model learned the internal relationship between the data, which then helped during the fine-tuning phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855203f3-b5fa-466c-bb01-bf422659455a",
   "metadata": {},
   "source": [
    "### b)\n",
    "\n",
    "#### Take a look at some of the correct predictions of the pretrain+finetuned vanilla model, as well as some of the errors. We think you’ll find that it’s impossible to tell, just looking at the output, whether the model retrieved the correct birth place, or made up an incorrect birth place. Consider the implications of this for user-facing systems that involve pretrained NLP components. Come up with two reasons why this indeterminacy of model behavior may cause concern for such applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f4c6e1-400b-4f79-933a-13aab79b4519",
   "metadata": {},
   "source": [
    "> There may be concerns that the model will adapt to the <ins>biases</ins> or <ins>stereotypes</ins> from the data on which it will be pretrained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648141f4-c9b1-471e-b755-7bfc9e98c288",
   "metadata": {},
   "source": [
    "### c)\n",
    "\n",
    "##### If your model didn’t see a person’s name at pretraining time, and that person was not seen at fine-tuning time either, it is not possible for it to have “learned” where they lived. Yet, your model will produce something as a predicted birth place for that person’s name if asked. Concisely describe a strategy your model might take for predicting a birth place for that person’s name, and one reason why this should cause concern for the use of such applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5a4850-1371-45ed-a3a8-25b35330b3db",
   "metadata": {},
   "source": [
    "> It would be helpful if the model was armed with a method of estimating its uncertainty, then the query could be further processed with high uncertainty for further verification. Concern is about biasing your data in situations such as choosing exotic locations for exotic names, which may be helpful in general, but it is not necessarily the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672784ba-4b75-49ef-8c5e-3ebdc68d8a10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tldl",
   "language": "python",
   "name": "tldl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
