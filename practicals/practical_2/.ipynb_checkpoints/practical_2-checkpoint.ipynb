{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "royal-account",
   "metadata": {},
   "source": [
    "Explanation:<br>\n",
    "$V$ - Vocabulary,<br>\n",
    "$U$ has vectors $u_w$ as rows,<br>\n",
    "$\\mathbf {1} _{A}(x)$ is a indicator function of subset $A$,<br>\n",
    "Operation $\\odot$ means pointwise multiplication,<br>\n",
    "Operation $ \\cdot $ is a dot product of vectors, or a scalar multiplication of a vector - depending on the context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-timing",
   "metadata": {},
   "source": [
    "# 1. The reasoning behind word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-walnut",
   "metadata": {},
   "source": [
    "\\begin{align} \\label{eq:erl}\n",
    "P(O=o|C=c) &= \\frac{exp(u_o^T \\cdot v_c)}{\\sum_{w \\in Vocab} exp(u_w^T\\cdot v_c)}, \\tag{1} \\\\\n",
    "J_{naive-softmax}(v_c , o, U) &= -\\log(P(O=o|C=c))    \\label{eq:erl1} \\tag{2}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressed-process",
   "metadata": {},
   "source": [
    "## a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-joseph",
   "metadata": {},
   "source": [
    "Show that the naive-softmax loss given in Equation (2) is equivalent to the cross-entropy loss\n",
    "between y and ŷ; i.e., show that:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-reference",
   "metadata": {},
   "source": [
    "$$ -\\sum_{w \\in Vocab} y_w \\cdot log(\\hat{y}_w) = -log(\\hat{y}_0)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-mileage",
   "metadata": {},
   "source": [
    "Note that $y_w$  , $\\hat{y}_w$ are vectors and $\\hat{y}_o$ is a scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-stanley",
   "metadata": {},
   "source": [
    "If $$\\hat{y}_o = P(O=o|C=c),$$ then $$\\log(\\hat{y}_0) = y^T \\cdot \\log(\\hat{y}) = \\sum_{w \\in V} y_w \\cdot \\log(\\hat{y}_w),$$<br>\n",
    "where $$y_w = \\mathbf {1} _{\\{o\\}}(w), \\hspace{1cm} y = [y_w]_{w \\in V}$$ and $$\\hat{y}_w = P(O=w|C=c), \\hspace{1cm} \\hat{y} = [\\hat{y}_w]_{w \\in V}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-immunology",
   "metadata": {},
   "source": [
    "## b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-salmon",
   "metadata": {},
   "source": [
    "Compute the partial derivative of $J_{naive-softmax}(v_c , o, U)$ with respect to $v_c$. Write your answer\n",
    "in terms of of $y$, $\\hat{y}$, and $U$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-ukraine",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "-\\frac{\\partial J_{naive-softmax}(v_c , o, U)}{\\partial v_c} &= \\frac{\\partial \\log(P(O=o|C=c))}{\\partial v_c} = \\frac{\\partial u_o^T \\cdot v_c}{\\partial v_c} - \\frac{\\partial \\log(\\sum_{w \\in Vocab} exp(u_w^T\\cdot v_c))}{\\partial v_c}\\\\&= u_o - \\frac{\\sum_{w \\in Vocab} exp(u_w^T\\cdot v_c)\\cdot u_w}{\\sum_{w \\in Vocab} exp(u_w^T\\cdot v_c)} = u_o - \\sum_{w \\in Vocab} \\frac{exp(u_w^T\\cdot v_c)}{\\sum_{w \\in Vocab} exp(u_w^T\\cdot v_c)}\\cdot u_w\\\\ &= u_o - U^T \\cdot \\hat{y}.\n",
    "\\end{align}\n",
    "\n",
    "Thus\n",
    "$$\\frac{\\partial J_{naive-softmax}(v_c , o, U)}{\\partial v_c} = U^T \\cdot \\hat{y} - u_o.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-gibson",
   "metadata": {},
   "source": [
    "## c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-homeless",
   "metadata": {},
   "source": [
    "Compute the partial derivatives of $J_{naive-softmax}(v_c , o, U)$ with respect to each of the outside\n",
    "word vectors, $u_w$ ’s. Write your answer in terms of $y$, $\\hat{y}$, and $v_c$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-irish",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "-\\frac{\\partial J_{naive-softmax}(v_c , o, U)}{\\partial u_{ws}} &= \\frac{\\partial \\log(P(O=o|C=c))}{\\partial u_{ws}} = \\frac{\\partial u_o^T \\cdot v_c}{\\partial u_{ws}} - \\frac{\\partial \\log(\\sum_{w \\in Vocab} exp(u_w^T\\cdot v_c))}{\\partial u_{ws}}\\\\ \n",
    "&= v_c\\cdot \\mathbf {1} _{\\{o\\}}(ws) - \\frac{ exp(u_ws^T\\cdot v_c)\\cdot v_c}{\\sum_{w \\in Vocab} exp(u_w^T\\cdot v_c)} = v_c \\cdot \\mathbf {1} _{\\{o\\}}(ws) - \\hat{y}_{ws} \\cdot v_c\n",
    "\\end{align}\n",
    "\n",
    "Thus\n",
    "$$\\frac{\\partial J_{naive-softmax}(v_c , o, U)}{\\partial v_{ws}} = (\\hat{y}_{ws} - \\mathbf {1} _{\\{o\\}}(ws)) \\cdot v_c.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-trinidad",
   "metadata": {},
   "source": [
    "## d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-nature",
   "metadata": {},
   "source": [
    "The sigmoid function is defined as:\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}} = \\frac{e^{x}}{1+e^{x}}$$\n",
    "\n",
    "Please compute the derivative of $\\sigma(x)$ with respect to $x$, where $x$ is a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-president",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "   \\frac{\\partial \\sigma(x)}{\\partial x} &= \\frac{\\partial e^x}{\\partial x} \\cdot \\frac{1+e^x}{(1+e^x)^2} - \\frac{\\partial 1+e^x}{\\partial x} \\cdot \\frac{e^x}{(1+e^x)^2}= \\frac{e^x}{(1+e^x)} - \\frac{e^{2x}}{(1+e^x)^2} = \\frac{e^x \\cdot 1}{(1+e^x)^2}\\\\ &= \\frac{e^x}{1+e^x} \\cdot \\frac{1 + e^x - e^x}{1+e^x} = \\frac{e^x}{1+e^x} \\cdot (1 - \\frac{e^x}{1+e^x})\\\\ &= \\sigma(x) \\cdot (1 - \\sigma(x))\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-posting",
   "metadata": {},
   "source": [
    "## e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-scott",
   "metadata": {},
   "source": [
    "Let’s consider the Negative Sampling loss, which is an alternative to the Naive Softmax loss.\n",
    "Assume that $K$ negative samples (words) are drawn from the vocabulary. For simplicity of\n",
    "notation we shall refer to them as $w_1 , w_2 , ..., w_K$ and their outside vectors as $u_1 , ..., u_K$. Note\n",
    "that $o \\notin  \\{w_1, ..., w_K\\}$. For a center word $c$ and an outside word $o$, the negative sampling loss\n",
    "function is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-devil",
   "metadata": {},
   "source": [
    "$$J_{neg-sample}(v_c , o, U) = -\\log(\\sigma(u_o^T\\cdot v_c)) - \\sum_{k=1}^K\\log(\\sigma(-u_k^T\\cdot v_c))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-leonard",
   "metadata": {},
   "source": [
    "for a sample $w_1 , ..., w_K$ , where $\\sigma(·)$ is the sigmoid function.\n",
    "Please repeat parts (b) and (c), computing the partial derivatives of $J_{neg-sample}$ with\n",
    "respect to $v_c$, with respect to $u_o$, and with respect to a negative sample $u_k$. Please write\n",
    "your answers in terms of the vectors $u_o$, $v_c$, and $u_k$, where $k \\in \\{1,.., K\\}$. After you’ve done this,\n",
    "describe with one sentence why this loss function is much more efficient to compute than the\n",
    "naive-softmax loss. Note, you should be able to use your solution to part (d) to help compute\n",
    "the necessary gradients here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-currency",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "-\\frac{\\partial J_{neg-sample}(v_c , o, U)}{\\partial v_c} \n",
    "&= \\frac{\\partial \\log(\\sigma(u_o^T\\cdot v_c))}{\\partial v_c} + \\frac{\\partial \\sum_{k=1}^K\\log(\\sigma(-u_k^T\\cdot v_c))}{\\partial v_c}\\\\\n",
    "&= \\frac{\\sigma(u_o^T\\cdot v_c)\\cdot (1 - \\sigma(u_o^T\\cdot v_c))}{\\sigma(u_o^T\\cdot v_c)}\\cdot u_o + \\sum_{k=1}^K\\frac{\\sigma(-u_k^T\\cdot v_c)\\cdot (1 - \\sigma(-u_k^T\\cdot v_c))}{\\sigma(-u_k^T\\cdot v_c)}) \\cdot (-u_k)\\\\\n",
    "&= (1 -\\sigma(u_o^T\\cdot v_c)) \\cdot u_o - \\sum_{k=1}^K(1-\\sigma(-u_k^T\\cdot v_c))\\cdot u_k\\\\\n",
    "&=(1 -\\sigma(u_o^T\\cdot v_c)) \\cdot u_o - \\sum_{k=1}^K\\sigma(u_k^T\\cdot v_c)\\cdot u_k, \\\\\n",
    "-\\frac{\\partial J_{neg-sample}(v_c , o, U)}{\\partial u_{o}} &= \\frac{\\partial \\log(\\sigma(u_o^T\\cdot v_c))}{\\partial u_{o}} + \\frac{\\partial \\sum_{k=1}^K\\log(\\sigma(-u_k^T\\cdot v_c))}{\\partial u_{o}} = (1 -\\sigma(u_o^T\\cdot v_c)) \\cdot v_c, \\\\\n",
    "-\\frac{\\partial J_{neg-sample}(v_c , o, U)}{\\partial u_{neg}} &= \\frac{\\partial \\log(\\sigma(u_o^T\\cdot v_c))}{\\partial u_{neg}} + \\frac{\\partial \\sum_{k=1}^K\\log(\\sigma(-u_k^T\\cdot v_c))}{\\partial u_{neg}} = -(1 -\\sigma(-u_{ks}^T\\cdot v_c)) \\cdot v_c = -\\sigma(u_{ks}^T\\cdot v_c) \\cdot v_c\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-advance",
   "metadata": {},
   "source": [
    "Thus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-acoustic",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\frac{\\partial J_{neg-sample}(v_c , o, U)}{\\partial v_c}  &= \\big(\\sum_{k=1}^K(1-\\sigma(-u_k^T\\cdot v_c))\\cdot u_k\\big) - (1 -\\sigma(u_o^T\\cdot v_c)) \\cdot u_o = \\big(\\sum_{k=1}^K\\sigma(u_k^T\\cdot v_c)\\cdot u_k\\big) - (1 -\\sigma(u_o^T\\cdot v_c)) \\cdot u_o, \\\\\n",
    "\\frac{\\partial J_{neg-sample}(v_c , o, U)}{\\partial u_{o}} &= (\\sigma(u_o^T\\cdot v_c) - 1) \\cdot v_c, \\\\\n",
    "\\frac{\\partial J_{neg-sample}(v_c , o, U)}{\\partial u_{neg}} &= (1 -\\sigma(-u_{ks}^T\\cdot v_c)) \\cdot v_c = \\sigma(u_{ks}^T\\cdot v_c) \\cdot v_c.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-payment",
   "metadata": {},
   "source": [
    "## f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-diploma",
   "metadata": {},
   "source": [
    "Suppose the center word is $c = w_t$ and the context window is $[w_{t-m}, ..., w_{t-1} , w_t , w_{t+1} , ..., w_{t+m}]$,\n",
    "where $m$ is the context window size. Recall that for the skip-gram version of word2vec, the\n",
    "total loss for the context window is:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-enclosure",
   "metadata": {},
   "source": [
    "$$\\Large J_{skip-gram}(v_c, w_{t-m}, ..., w_{t+m}, U) = \\sum_{-m\\leq j \\leq m, j \\neq 0}J_{skip-gram}(v_c, w_{t+j}, U)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-automation",
   "metadata": {},
   "source": [
    "Here, $J_{skip-gram}(v_c, w_{t+j}, U)$ represents an arbitrary loss term for the center word $c = w_t$ and\n",
    "outside word $w_{t+j}$. $J_{skip-gram}(v_c, w_{t+j}, U)$ could be $J_{neg-sample}(v_c, w_{t+j}, U)$ or $J_{naive-softmax}(v_c, w_{t+j}, U)$, depending on your implementation. Write down three partial derivatives:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-alignment",
   "metadata": {},
   "source": [
    "1. $\\Large\\frac{\\partial J_{skip-gram}(v_c, w_{t+j}, U)}{\\partial U} = \\sum_{-m\\leq j \\leq m, j \\neq 0} \\frac{\\partial J_{skip-gram}(v_c, w_{t+j}, U)}{\\partial U}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-walker",
   "metadata": {},
   "source": [
    "2. $\\Large\\frac{\\partial J_{skip-gram}(v_c, w_{t+j}, U)}{\\partial v_c} = \\sum_{-m\\leq j \\leq m, j \\neq 0} \\frac{\\partial J_{skip-gram}(v_c, w_{t+j}, U)}{\\partial v_c}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-handbook",
   "metadata": {},
   "source": [
    "3. $\\Large\\frac{\\partial J_{skip-gram}(v_c, w_{t+j}, U)}{\\partial w_i} = \\sum_{-m\\leq j \\leq m, j \\neq 0} \\frac{\\partial J_{skip-gram}(v_c, w_{t+j}, U)}{\\partial w_i}$, where $-m\\leq i \\leq m, i \\neq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-forwarding",
   "metadata": {},
   "source": [
    "Write your answers in terms of"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-roads",
   "metadata": {},
   "source": [
    "$\\Large\\frac{\\partial J_{skip-gram}(v_c, w_{t+j}, U)}{\\partial U}$ and $\\Large\\frac{\\partial J_{skip-gram}(v_c, w_{t+j}, U)}{\\partial v_c}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-article",
   "metadata": {},
   "source": [
    "# 2. Building your version of word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-monte",
   "metadata": {},
   "source": [
    "## a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "checked-boundary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Gradient check for skip-gram with naive_softmax_loss_and_gradient ====\n",
      "Gradient check passed!\n",
      "==== Gradient check for skip-gram with neg_sampling_loss_and_gradient ====\n",
      "Gradient check passed!\n",
      "\n",
      "=== Results ===\n",
      "Skip-Gram with naive_softmax_loss_and_gradient\n",
      "Your Result:\n",
      "Loss: 11.16610900153398\n",
      "Gradient wrt Center Vectors (dJ/dV):\n",
      " [[ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [-1.26947339 -1.36873189  2.45158957]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      "Gradient wrt Outside Vectors (dJ/dU):\n",
      " [[-0.41045956  0.18834851  1.43272264]\n",
      " [ 0.38202831 -0.17530219 -1.33348241]\n",
      " [ 0.07009355 -0.03216399 -0.24466386]\n",
      " [ 0.09472154 -0.04346509 -0.33062865]\n",
      " [-0.13638384  0.06258276  0.47605228]]\n",
      "\n",
      "Expected Result: Value should approximate these:\n",
      "Loss: 11.16610900153398\n",
      "Gradient wrt Center Vectors (dJ/dV):\n",
      " [[ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [-1.26947339 -1.36873189  2.45158957]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      "Gradient wrt Outside Vectors (dJ/dU):\n",
      " [[-0.41045956  0.18834851  1.43272264]\n",
      " [ 0.38202831 -0.17530219 -1.33348241]\n",
      " [ 0.07009355 -0.03216399 -0.24466386]\n",
      " [ 0.09472154 -0.04346509 -0.33062865]\n",
      " [-0.13638384  0.06258276  0.47605228]]\n",
      "    \n",
      "Skip-Gram with neg_sampling_loss_and_gradient\n",
      "Your Result:\n",
      "Loss: 16.15119285363322\n",
      "Gradient wrt Center Vectors (dJ/dV):\n",
      " [[ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [-4.54650789 -1.85942252  0.76397441]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      " Gradient wrt Outside Vectors (dJ/dU):\n",
      " [[-0.69148188  0.31730185  2.41364029]\n",
      " [-0.22716495  0.10423969  0.79292674]\n",
      " [-0.45528438  0.20891737  1.58918512]\n",
      " [-0.31602611  0.14501561  1.10309954]\n",
      " [-0.80620296  0.36994417  2.81407799]]\n",
      "\n",
      "Expected Result: Value should approximate these:\n",
      "Loss: 16.15119285363322\n",
      "Gradient wrt Center Vectors (dJ/dV):\n",
      " [[ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [-4.54650789 -1.85942252  0.76397441]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      " Gradient wrt Outside Vectors (dJ/dU):\n",
      " [[-0.69148188  0.31730185  2.41364029]\n",
      " [-0.22716495  0.10423969  0.79292674]\n",
      " [-0.45528438  0.20891737  1.58918512]\n",
      " [-0.31602611  0.14501561  1.10309954]\n",
      " [-0.80620296  0.36994417  2.81407799]]\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "! python3 practical_2/word2vec.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepting-anxiety",
   "metadata": {},
   "source": [
    "## b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "alike-justice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sanity checks...\n",
      "iter 100: 0.004578\n",
      "iter 200: 0.004353\n",
      "iter 300: 0.004136\n",
      "iter 400: 0.003929\n",
      "iter 500: 0.003733\n",
      "iter 600: 0.003546\n",
      "iter 700: 0.003369\n",
      "iter 800: 0.003200\n",
      "iter 900: 0.003040\n",
      "iter 1000: 0.002888\n",
      "test 1 result: 8.414836786079764e-10\n",
      "iter 100: 0.000000\n",
      "iter 200: 0.000000\n",
      "iter 300: 0.000000\n",
      "iter 400: 0.000000\n",
      "iter 500: 0.000000\n",
      "iter 600: 0.000000\n",
      "iter 700: 0.000000\n",
      "iter 800: 0.000000\n",
      "iter 900: 0.000000\n",
      "iter 1000: 0.000000\n",
      "test 2 result: 0.0\n",
      "iter 100: 0.041205\n",
      "iter 200: 0.039181\n",
      "iter 300: 0.037222\n",
      "iter 400: 0.035361\n",
      "iter 500: 0.033593\n",
      "iter 600: 0.031913\n",
      "iter 700: 0.030318\n",
      "iter 800: 0.028802\n",
      "iter 900: 0.027362\n",
      "iter 1000: 0.025994\n",
      "test 3 result: -2.524451035823933e-09\n",
      "----------------------------------------\n",
      "ALL TESTS PASSED\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "! python3 practical_2/sgd.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-thriller",
   "metadata": {},
   "source": [
    "## c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "responsible-identifier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 200: 20.891404\n",
      "iter 400: 20.967651\n",
      "iter 600: 21.040115\n",
      "iter 800: 21.078435\n",
      "iter 1000: 21.069096\n",
      "iter 1200: 21.060170\n",
      "iter 1400: 21.242247\n",
      "iter 1600: 21.392376\n",
      "iter 1800: 21.374748\n",
      "iter 2000: 21.182861\n",
      "iter 2200: 21.343109\n",
      "iter 2400: 21.334666\n",
      "iter 2600: 21.433110\n",
      "iter 2800: 21.538466\n",
      "iter 3000: 21.572961\n",
      "iter 3200: 21.613596\n",
      "iter 3400: 21.667252\n",
      "iter 3600: 21.672564\n",
      "iter 3800: 21.629025\n",
      "iter 4000: 21.594488\n",
      "iter 4200: 21.643039\n",
      "iter 4400: 21.699231\n",
      "iter 4600: 21.629614\n",
      "iter 4800: 21.830132\n",
      "iter 5000: 21.905360\n",
      "iter 5200: 21.897115\n",
      "iter 5400: 21.864837\n",
      "iter 5600: 21.834640\n",
      "iter 5800: 21.626398\n",
      "iter 6000: 21.605394\n",
      "iter 6200: 21.533552\n",
      "iter 6400: 21.373265\n",
      "iter 6600: 21.214994\n",
      "iter 6800: 21.086913\n",
      "iter 7000: 20.910547\n",
      "iter 7200: 20.754043\n",
      "iter 7400: 20.609923\n",
      "iter 7600: 20.629928\n",
      "iter 7800: 20.402046\n",
      "iter 8000: 20.348291\n",
      "iter 8200: 20.258341\n",
      "iter 8400: 20.071469\n",
      "iter 8600: 19.841807\n",
      "iter 8800: 19.769863\n",
      "iter 9000: 19.567216\n",
      "iter 9200: 19.517002\n",
      "iter 9400: 19.410352\n",
      "iter 9600: 19.364466\n",
      "iter 9800: 19.252376\n",
      "iter 10000: 19.159998\n",
      "iter 10200: 19.031972\n",
      "iter 10400: 18.802428\n",
      "iter 10600: 18.658284\n",
      "iter 10800: 18.496896\n",
      "iter 11000: 18.199716\n",
      "iter 11200: 18.072541\n",
      "iter 11400: 17.930553\n",
      "iter 11600: 17.784590\n",
      "iter 11800: 17.654833\n",
      "iter 12000: 17.683665\n",
      "iter 12200: 17.612869\n",
      "iter 12400: 17.549928\n",
      "iter 12600: 17.390123\n",
      "iter 12800: 17.139387\n",
      "iter 13000: 17.062908\n",
      "iter 13200: 16.957244\n",
      "iter 13400: 16.734267\n",
      "iter 13600: 16.678865\n",
      "iter 13800: 16.628049\n",
      "iter 14000: 16.474059\n",
      "iter 14200: 16.321952\n",
      "iter 14400: 16.182363\n",
      "iter 14600: 15.934935\n",
      "iter 14800: 15.905041\n",
      "iter 15000: 15.790566\n",
      "iter 15200: 15.740665\n",
      "iter 15400: 15.626116\n",
      "iter 15600: 15.515085\n",
      "iter 15800: 15.403771\n",
      "iter 16000: 15.315924\n",
      "iter 16200: 15.208684\n",
      "iter 16400: 15.129671\n",
      "iter 16600: 15.035023\n",
      "iter 16800: 14.932070\n",
      "iter 17000: 14.779587\n",
      "iter 17200: 14.594729\n",
      "iter 17400: 14.482813\n",
      "iter 17600: 14.438689\n",
      "iter 17800: 14.376448\n",
      "iter 18000: 14.267814\n",
      "iter 18200: 14.234439\n",
      "iter 18400: 14.152104\n",
      "iter 18600: 14.073947\n",
      "iter 18800: 14.011053\n",
      "iter 19000: 13.957537\n",
      "iter 19200: 13.932363\n",
      "iter 19400: 13.762001\n",
      "iter 19600: 13.785834\n",
      "iter 19800: 13.798168\n",
      "iter 20000: 13.652115\n",
      "iter 20200: 13.507294\n",
      "iter 20400: 13.382248\n",
      "iter 20600: 13.260731\n",
      "iter 20800: 13.190741\n",
      "iter 21000: 13.144332\n",
      "iter 21200: 13.046970\n",
      "iter 21400: 13.021142\n",
      "iter 21600: 12.873439\n",
      "iter 21800: 12.837046\n",
      "iter 22000: 12.744611\n",
      "iter 22200: 12.671654\n",
      "iter 22400: 12.637048\n",
      "iter 22600: 12.549126\n",
      "iter 22800: 12.437774\n",
      "iter 23000: 12.385856\n",
      "iter 23200: 12.420570\n",
      "iter 23400: 12.379635\n",
      "iter 23600: 12.301323\n",
      "iter 23800: 12.279863\n",
      "iter 24000: 12.264092\n",
      "iter 24200: 12.214453\n",
      "iter 24400: 12.178568\n",
      "iter 24600: 12.144627\n",
      "iter 24800: 12.115822\n",
      "iter 25000: 12.133124\n",
      "iter 25200: 12.120576\n",
      "iter 25400: 12.020635\n",
      "iter 25600: 12.019060\n",
      "iter 25800: 11.961826\n",
      "iter 26000: 11.981944\n",
      "iter 26200: 11.949396\n",
      "iter 26400: 11.953021\n",
      "iter 26600: 12.045901\n",
      "iter 26800: 11.963473\n",
      "iter 27000: 11.903870\n",
      "iter 27200: 11.871640\n",
      "iter 27400: 11.762743\n",
      "iter 27600: 11.831252\n",
      "iter 27800: 11.785822\n",
      "iter 28000: 11.696140\n",
      "iter 28200: 11.670252\n",
      "iter 28400: 11.646027\n",
      "iter 28600: 11.578997\n",
      "iter 28800: 11.604102\n",
      "iter 29000: 11.610373\n",
      "iter 29200: 11.531222\n",
      "iter 29400: 11.550374\n",
      "iter 29600: 11.490591\n",
      "iter 29800: 11.440893\n",
      "iter 30000: 11.399276\n",
      "iter 30200: 11.437003\n",
      "iter 30400: 11.370494\n",
      "iter 30600: 11.329350\n",
      "iter 30800: 11.325113\n",
      "iter 31000: 11.299174\n",
      "iter 31200: 11.313634\n",
      "iter 31400: 11.336354\n",
      "iter 31600: 11.264844\n",
      "iter 31800: 11.278345\n",
      "iter 32000: 11.225357\n",
      "iter 32200: 11.164029\n",
      "iter 32400: 11.099900\n",
      "iter 32600: 11.114280\n",
      "iter 32800: 11.112207\n",
      "iter 33000: 11.133023\n",
      "iter 33200: 11.078268\n",
      "iter 33400: 11.094497\n",
      "iter 33600: 11.157818\n",
      "iter 33800: 11.190636\n",
      "iter 34000: 11.189956\n",
      "iter 34200: 11.240002\n",
      "iter 34400: 11.181405\n",
      "iter 34600: 11.183337\n",
      "iter 34800: 11.166485\n",
      "iter 35000: 11.156603\n",
      "iter 35200: 11.159042\n",
      "iter 35400: 11.176093\n",
      "iter 35600: 11.110668\n",
      "iter 35800: 11.007259\n",
      "iter 36000: 10.999718\n",
      "iter 36200: 10.930412\n",
      "iter 36400: 10.908094\n",
      "iter 36600: 10.930568\n",
      "iter 36800: 10.972314\n",
      "iter 37000: 10.908679\n",
      "iter 37200: 10.809657\n",
      "iter 37400: 10.833767\n",
      "iter 37600: 10.828671\n",
      "iter 37800: 10.781401\n",
      "iter 38000: 10.710367\n",
      "iter 38200: 10.735919\n",
      "iter 38400: 10.748470\n",
      "iter 38600: 10.801216\n",
      "iter 38800: 10.749743\n",
      "iter 39000: 10.713716\n",
      "iter 39200: 10.806123\n",
      "iter 39400: 10.755537\n",
      "iter 39600: 10.737002\n",
      "iter 39800: 10.688723\n",
      "iter 40000: 10.712872\n",
      "sanity check: cost at convergence should be around or below 10\n",
      "training took 10322 seconds\n"
     ]
    }
   ],
   "source": [
    "! python3 practical_2/run.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-display",
   "metadata": {},
   "source": [
    "![alt text](word_vectors.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detected-diana",
   "metadata": {},
   "source": [
    "From the embeddings projected to two dimensions, I can observe that there are groups of words which describe emotional states, or terms that can describe a person (zły, świetny), (wspaniały, głupi) or general terms (dobrze, spoko), and a group of grades military (generał, sierżant). However, for each of these groups there is an element remote from these groups. It is respectively nudny, ekstra, and porucznik. Longer training should lead to more distinct clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-agency",
   "metadata": {},
   "source": [
    "# 3. Optimizing Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-qatar",
   "metadata": {},
   "source": [
    "## a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-hierarchy",
   "metadata": {},
   "source": [
    "The process of updating the weights uses the current 'momentum' of the process to maintain the trajectories of previous changes. Despite the sudden change of direction, this process will take into account the direction taken in earlier updates through the exponential moving average. Thanks to this, not only the currently collected suggestion on the best directions for improving the loss function is taken into account, but also previous suggestions on this subject. This treatment provides updates, that are not as varied as plain SGD, by keeping the mentioned direction. Accounting for history of direction changes takes into account previous batches not just a current batch, which may be biased, and thus may improve the quality of generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-hostel",
   "metadata": {},
   "source": [
    "## b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-direction",
   "metadata": {},
   "source": [
    "Since $v$ accumulates the squares of local weight changes (derivatives), in the learning process, through the exponential moving average, the larger update will have weights whose squares of local weight changes will be getting smaller. This action should (in its conception) prevent the gradient from fading prematurely, by shifting the weights around the current position, or prevent from falling into shallow local extremes, ravines or saddles. In general, it aligns the learning rate with the learning stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-equipment",
   "metadata": {},
   "source": [
    "## c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-nepal",
   "metadata": {},
   "source": [
    "More data reduces the variance of the model's response, therefore, when fewer data are available, the reduction of this variance can be obtained by using L2 regularization, which reduces the model weights.\n",
    "\n",
    "In the case of the Adam algorithm, the authors show that L2 regularization is not the same as weight decay regularization, as is the case when the SGD algorithm is used to update the weights. The authors propose to separate the weight decay factor from gradient based update, i.e. introduce the weight decay factor in the last part of the update in order not to associate it with the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-laugh",
   "metadata": {},
   "source": [
    "## d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-nothing",
   "metadata": {},
   "source": [
    "We should not use the dropout during the evaluation phase as we want to use all potentially learned neurons during the training phase. Collectively, all neurons have the right to influence the final result. We can apply dropout during training phase because, a network can then learn to use different systems of neurons to recognize similar systems of features, thus preventing a phenomenon called complex co-adaptations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-ceramic",
   "metadata": {},
   "source": [
    "You can look at dropout regularization as training multiple diverse models at the same time. As a result, we get an ensemble of models (in fact a variational approximation method) that we can use to estimate the final result, and parameters such as the posterior mean or the posterior variation. Therefore, using a dropout, we can estimate the uncertainty about the result obtained by such an ensemble of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-stations",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tldl",
   "language": "python",
   "name": "tldl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
